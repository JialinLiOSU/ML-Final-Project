{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_learning_VGG16.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Fwu5060GYClx","colab_type":"code","outputId":"f6e05511-e22d-426e-a71c-1cc76394344f","executionInfo":{"status":"ok","timestamp":1555522886086,"user_tz":240,"elapsed":4371,"user":{"displayName":"Jialin Li","photoUrl":"","userId":"02457601019262499397"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from __future__ import print_function\n","\n","import numpy as np\n","import warnings\n","import os\n","import time\n","import keras\n","\n","from keras.models import Model\n","from keras.layers import Activation\n","from keras.layers import Flatten\n","from keras.layers import Dense\n","from keras.layers import Input\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import GlobalMaxPooling2D\n","from keras.layers import GlobalAveragePooling2D\n","from keras.layers import merge, Input\n","from keras.preprocessing import image\n","from keras.utils import layer_utils\n","from keras.utils.data_utils import get_file\n","from keras.utils import np_utils\n","from keras import backend as K\n","from keras.applications.imagenet_utils import decode_predictions\n","from keras.applications.imagenet_utils import preprocess_input\n","from keras_applications.imagenet_utils import _obtain_input_shape\n","from keras.engine.topology import get_source_inputs\n","from sklearn.utils import shuffle\n","from sklearn.model_selection  import train_test_split"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"QVAodt6RYMLS","colab_type":"code","colab":{}},"cell_type":"code","source":["WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n","WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n","\n","\n","def VGG16(include_top=True, weights='imagenet',\n","          input_tensor=None, input_shape=None,\n","          pooling=None,\n","          classes=1000):\n","    \"\"\"Instantiates the VGG16 architecture.\n","\n","    Optionally loads weights pre-trained\n","    on ImageNet. Note that when using TensorFlow,\n","    for best performance you should set\n","    `image_data_format=\"channels_last\"` in your Keras config\n","    at ~/.keras/keras.json.\n","\n","    The model and the weights are compatible with both\n","    TensorFlow and Theano. The data format\n","    convention used by the model is the one\n","    specified in your Keras config file.\n","\n","    # Arguments\n","        include_top: whether to include the 3 fully-connected\n","            layers at the top of the network.\n","        weights: one of `None` (random initialization)\n","            or \"imagenet\" (pre-training on ImageNet).\n","        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n","            to use as image input for the model.\n","        input_shape: optional shape tuple, only to be specified\n","            if `include_top` is False (otherwise the input shape\n","            has to be `(224, 224, 3)` (with `channels_last` data format)\n","            or `(3, 224, 244)` (with `channels_first` data format).\n","            It should have exactly 3 inputs channels,\n","            and width and height should be no smaller than 48.\n","            E.g. `(200, 200, 3)` would be one valid value.\n","        pooling: Optional pooling mode for feature extraction\n","            when `include_top` is `False`.\n","            - `None` means that the output of the model will be\n","                the 4D tensor output of the\n","                last convolutional layer.\n","            - `avg` means that global average pooling\n","                will be applied to the output of the\n","                last convolutional layer, and thus\n","                the output of the model will be a 2D tensor.\n","            - `max` means that global max pooling will\n","                be applied.\n","        classes: optional number of classes to classify images\n","            into, only to be specified if `include_top` is True, and\n","            if no `weights` argument is specified.\n","\n","    # Returns\n","        A Keras model instance.\n","\n","    # Raises\n","        ValueError: in case of invalid argument for `weights`,\n","            or invalid input shape.\n","    \"\"\"\n","    if weights not in {'imagenet', None}:\n","        raise ValueError('The `weights` argument should be either '\n","                         '`None` (random initialization) or `imagenet` '\n","                         '(pre-training on ImageNet).')\n","\n","    if weights == 'imagenet' and include_top and classes != 1000:\n","        raise ValueError('If using `weights` as imagenet with `include_top`'\n","                         ' as true, `classes` should be 1000')\n","    # Determine proper input shape\n","    input_shape = _obtain_input_shape(input_shape,\n","                                      default_size=224,\n","                                      min_size=48,\n","                                      data_format=K.image_data_format(),\n","                                      require_flatten=include_top)\n","\n","    if input_tensor is None:\n","        img_input = Input(shape=input_shape)\n","    else:\n","        if not K.is_keras_tensor(input_tensor):\n","            img_input = Input(tensor=input_tensor, shape=input_shape)\n","        else:\n","            img_input = input_tensor\n","    # Block 1\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n","\n","    # Block 2\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n","\n","    # Block 3\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n","\n","    # Block 4\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n","\n","    # Block 5\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n","\n","    if include_top:\n","        # Classification block\n","        x = Flatten(name='flatten')(x)\n","        x = Dense(4096, activation='relu', name='fc1')(x)\n","        x = Dense(4096, activation='relu', name='fc2')(x)\n","        x = Dense(classes, activation='softmax', name='predictions')(x)\n","    else:\n","        if pooling == 'avg':\n","            x = GlobalAveragePooling2D()(x)\n","        elif pooling == 'max':\n","            x = GlobalMaxPooling2D()(x)\n","\n","    # Ensure that the model takes into account\n","    # any potential predecessors of `input_tensor`.\n","    if input_tensor is not None:\n","        inputs = get_source_inputs(input_tensor)\n","    else:\n","        inputs = img_input\n","    # Create model.\n","    model = Model(inputs, x, name='vgg16')\n","\n","    # load weights\n","    if weights == 'imagenet':\n","        if include_top:\n","            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n","                                    WEIGHTS_PATH,\n","                                    cache_subdir='models')\n","        else:\n","            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n","                                    WEIGHTS_PATH_NO_TOP,\n","                                    cache_subdir='models')\n","        model.load_weights(weights_path)\n","        if K.backend() == 'theano':\n","            layer_utils.convert_all_kernels_in_model(model)\n","\n","        if K.image_data_format() == 'channels_first':\n","            if include_top:\n","                maxpool = model.get_layer(name='block5_pool')\n","                shape = maxpool.output_shape[1:]\n","                dense = model.get_layer(name='fc1')\n","                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n","\n","            if K.backend() == 'tensorflow':\n","                warnings.warn('You are using the TensorFlow backend, yet you '\n","                              'are using the Theano '\n","                              'image data format convention '\n","                              '(`image_data_format=\"channels_first\"`). '\n","                              'For best performance, set '\n","                              '`image_data_format=\"channels_last\"` in '\n","                              'your Keras config '\n","                              'at ~/.keras/keras.json.')\n","    return model\n","\n","\n","# if __name__ == '__main__':\n","#     model = VGG16(include_top=True, weights='imagenet')\n","\n","#     img_path = 'elephant.jpg'\n","#     img = image.load_img(img_path, target_size=(224, 224))\n","#     x = image.img_to_array(img)\n","#     x = np.expand_dims(x, axis=0)\n","#     x = preprocess_input(x)\n","#     print('Input image shape:', x.shape)\n","\n","#     preds = model.predict(x)\n","#     print('Predicted:', decode_predictions(preds))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c4x3QTKTVhHB","colab_type":"code","outputId":"d346fed7-42da-4aad-9e92-afc296532d92","executionInfo":{"status":"ok","timestamp":1555522886094,"user_tz":240,"elapsed":4366,"user":{"displayName":"Jialin Li","photoUrl":"","userId":"02457601019262499397"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"sIDUFY6XKpEc","colab_type":"code","colab":{}},"cell_type":"code","source":["# Loading the training data\n","# PATH = os.getcwd()\n","# Define data path\n","data_path = '/content/drive/My Drive/region map'\n","data_dir_list = os.listdir(data_path)     "],"execution_count":0,"outputs":[]},{"metadata":{"id":"g1aQTqBzLnQ7","colab_type":"code","outputId":"39762295-1180-4bf0-8deb-72c66c965f4a","executionInfo":{"status":"ok","timestamp":1555523055496,"user_tz":240,"elapsed":173754,"user":{"displayName":"Jialin Li","photoUrl":"","userId":"02457601019262499397"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["img_data_list=[]\n","\n","for dataset in data_dir_list:\n","\timg_list=os.listdir(data_path+'/'+ dataset)\n","\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n","\tfor img in img_list:\n","\t\timg_path = data_path + '/'+ dataset + '/'+ img\n","\t\timg = image.load_img(img_path, target_size=(224, 224))\n","\t\tx = image.img_to_array(img)\n","\t\tx = np.expand_dims(x, axis=0)\n","\t\tx = preprocess_input(x)\n","# \t\tx = x/255\n","# \t\tprint('Input image shape:', x.shape)\n","\t\timg_data_list.append(x)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Loaded the images of dataset-china\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/PIL/Image.py:914: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n","  'to RGBA images')\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded the images of dataset-south_korea\n","\n","Loaded the images of dataset-us\n","\n","Loaded the images of dataset-world\n","\n"],"name":"stdout"}]},{"metadata":{"id":"M2gODdgqXFTG","colab_type":"code","outputId":"f1f3da77-91ea-485b-c237-18d171f1c8ea","executionInfo":{"status":"ok","timestamp":1555523059763,"user_tz":240,"elapsed":178016,"user":{"displayName":"Jialin Li","photoUrl":"","userId":"02457601019262499397"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["img_data = np.array(img_data_list)\n","#img_data = img_data.astype('float32')\n","print (img_data.shape)\n","img_data=np.rollaxis(img_data,1,0)\n","print (img_data.shape)\n","img_data=img_data[0]\n","print (img_data.shape)\n","\n","#Training the feature extraction also\n","batch_size=32   #change batch_size\n","# epochs_list=[20,40,60,80,100]\n","epochs=20\n","layer_inx_list=[-3,-4,-5,-6,-7,-8,-9,-10]\n","\n","# Define the number of classes\n","num_classes = 4\n","num_img_each_total=1500\n","num_img_each_list=[i for i in range(100,1500,200)] # 2"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(6000, 1, 224, 224, 3)\n","(1, 6000, 224, 224, 3)\n","(6000, 224, 224, 3)\n"],"name":"stdout"}]},{"metadata":{"id":"iAfWRxnJXnA5","colab_type":"code","outputId":"7f461f73-54aa-4d70-b42f-d17ff1550e76","executionInfo":{"status":"ok","timestamp":1555523071692,"user_tz":240,"elapsed":189941,"user":{"displayName":"Jialin Li","photoUrl":"","userId":"02457601019262499397"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["num_img_each=1500\n","num_of_samples = num_img_each*num_classes\n","labels = np.ones((num_of_samples,),dtype='int64')\n","X1=img_data[0:num_img_each]\n","X2=img_data[num_img_each_total:num_img_each_total+num_img_each]\n","X3=img_data[num_img_each_total*2:num_img_each_total*2+num_img_each]\n","X4=img_data[num_img_each_total*3:num_img_each_total*3+num_img_each]\n","X=np.concatenate((X1,X2,X3,X4),axis=0)\n","\n","labels[0:num_img_each]=0\n","labels[num_img_each:num_img_each*2]=1\n","labels[num_img_each*2:num_img_each*3]=2\n","labels[num_img_each*3:num_img_each*4]=3\n","del img_data, img_data_list,X1,X2,X3,X4\n","# names = ['china','south_korea','us','world']\n","# convert class labels to on-hot encoding\n","Y = np_utils.to_categorical(labels, num_classes)\n","#Shuffle the dataset\n","x,y = shuffle(X,Y, random_state=2)\n","# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n","print (\"num_img_each:\",num_img_each)\n","del X,Y,x,y"],"execution_count":7,"outputs":[{"output_type":"stream","text":["num_img_each: 1500\n"],"name":"stdout"}]},{"metadata":{"id":"jBYQkru-XtFq","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BWoLI_ymmBUk","colab_type":"code","outputId":"59459248-7620-4248-dcf0-98811e8f61d7","executionInfo":{"status":"ok","timestamp":1555528395632,"user_tz":240,"elapsed":1715321,"user":{"displayName":"Jialin Li","photoUrl":"","userId":"02457601019262499397"}},"colab":{"base_uri":"https://localhost:8080/","height":13447}},"cell_type":"code","source":["for layer_inx in layer_inx_list:\n","    image_input = Input(shape=(224, 224, 3))\n","    model = VGG16(input_tensor=image_input, include_top=True,weights='imagenet')\n","    print (\"layer_inx:\",layer_inx)\n","        \n","\n","    last_layer = model.get_layer('block5_pool').output\n","    x= Flatten(name='flatten')(last_layer)\n","    x = Dense(128, activation='relu', name='fc1')(x)\n","    x = Dense(128, activation='relu', name='fc2')(x)\n","    out = Dense(num_classes, activation='softmax', name='output')(x)\n","    custom_vgg_model2 = Model(image_input, out)\n","\n","    # freeze all the layers except the dense layers\n","    for layer in custom_vgg_model2.layers[:layer_inx]:\n","        layer.trainable = False\n","            \n","    custom_vgg_model2.summary()\n","        \n","    custom_vgg_model2.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n","\n","    t=time.time()\n","    #\tt = now()\n","    hist = custom_vgg_model2.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n","    print('Training time: %s' % (time.time()-t))\n","    (loss, accuracy) = custom_vgg_model2.evaluate(X_test, y_test, batch_size=10, verbose=1)\n","\n","    print()\n","    str1=\"Batch_size: \"+str(batch_size)+' epochs: '+str(epochs)+'\\n'\n","    str2=\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100)+'\\n'\n","    str3='Training time: ' + str (time.time()-t)+'\\n'\n","    str4='num_img_each: ' + str (num_img_each)+'\\n'\n","    str5='layer_inx: ' + str (layer_inx)+'\\n'\n","    data_path = '/content/drive/My Drive/Experiment results/'\n","    filename='Experiment_results_pretrained_6000'+'.txt'\n","    filename=data_path+ filename\n","    file = open(filename,'a')\n","    file.write(str4) \n","    file.write(str5)\n","    file.write(str1) \n","    file.write(str2)\n","    file.write(str3)\n","    file.close()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","layer_inx: -3\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 3,228,420\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 36s 8ms/step - loss: 12.0131 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 2/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 8.0895 - acc: 0.4923 - val_loss: 5.5620 - val_acc: 0.6458\n","Epoch 3/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.6207 - acc: 0.7102 - val_loss: 5.2425 - val_acc: 0.6717\n","Epoch 4/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 4.3088 - acc: 0.7306 - val_loss: 4.8810 - val_acc: 0.6958\n","Epoch 5/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.3432 - acc: 0.7285 - val_loss: 4.6450 - val_acc: 0.7108\n","Epoch 6/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2228 - acc: 0.7367 - val_loss: 4.6476 - val_acc: 0.7075\n","Epoch 7/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2457 - acc: 0.7356 - val_loss: 4.5012 - val_acc: 0.7200\n","Epoch 8/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2351 - acc: 0.7367 - val_loss: 4.5108 - val_acc: 0.7200\n","Epoch 9/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2900 - acc: 0.7325 - val_loss: 4.6561 - val_acc: 0.7100\n","Epoch 10/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1386 - acc: 0.7419 - val_loss: 4.6425 - val_acc: 0.7108\n","Epoch 11/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1973 - acc: 0.7394 - val_loss: 4.6804 - val_acc: 0.7083\n","Epoch 12/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1448 - acc: 0.7425 - val_loss: 4.5361 - val_acc: 0.7183\n","Epoch 13/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1154 - acc: 0.7446 - val_loss: 4.4191 - val_acc: 0.7258\n","Epoch 14/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1306 - acc: 0.7433 - val_loss: 4.5265 - val_acc: 0.7192\n","Epoch 15/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1167 - acc: 0.7444 - val_loss: 4.5351 - val_acc: 0.7175\n","Epoch 16/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1260 - acc: 0.7435 - val_loss: 4.4459 - val_acc: 0.7242\n","Epoch 17/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.0795 - acc: 0.7465 - val_loss: 4.5121 - val_acc: 0.7183\n","Epoch 18/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.0668 - acc: 0.7475 - val_loss: 4.4728 - val_acc: 0.7225\n","Epoch 19/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.0698 - acc: 0.7475 - val_loss: 4.4728 - val_acc: 0.7225\n","Epoch 20/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.0698 - acc: 0.7475 - val_loss: 4.4728 - val_acc: 0.7225\n","Training time: 617.6545741558075\n","1200/1200 [==============================] - 9s 8ms/step\n","\n","layer_inx: -4\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 3,228,420\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1312 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 2/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 3/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 4/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 5/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 6/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 7/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 8/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 9/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 10/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 11/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 12/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 13/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 14/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 15/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 16/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 17/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 18/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 19/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 20/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 12.1557 - acc: 0.2458 - val_loss: 11.8065 - val_acc: 0.2675\n","Training time: 611.4373240470886\n","1200/1200 [==============================] - 8s 6ms/step\n","\n","layer_inx: -5\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 3,228,420\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 8.6694 - acc: 0.4550 - val_loss: 7.9884 - val_acc: 0.5042\n","Epoch 2/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 6.1677 - acc: 0.6131 - val_loss: 4.6731 - val_acc: 0.7067\n","Epoch 3/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.5216 - acc: 0.7167 - val_loss: 4.4282 - val_acc: 0.7233\n","Epoch 4/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.5542 - acc: 0.7156 - val_loss: 4.2833 - val_acc: 0.7333\n","Epoch 5/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.4865 - acc: 0.7204 - val_loss: 4.2256 - val_acc: 0.7367\n","Epoch 6/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.3935 - acc: 0.7269 - val_loss: 4.1328 - val_acc: 0.7425\n","Epoch 7/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.3282 - acc: 0.7313 - val_loss: 4.7505 - val_acc: 0.7025\n","Epoch 8/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.3555 - acc: 0.7294 - val_loss: 4.2699 - val_acc: 0.7325\n","Epoch 9/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.3321 - acc: 0.7308 - val_loss: 4.2986 - val_acc: 0.7325\n","Epoch 10/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.3054 - acc: 0.7323 - val_loss: 4.1832 - val_acc: 0.7400\n","Epoch 11/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.3047 - acc: 0.7325 - val_loss: 4.2021 - val_acc: 0.7392\n","Epoch 12/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2368 - acc: 0.7369 - val_loss: 4.1695 - val_acc: 0.7400\n","Epoch 13/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2575 - acc: 0.7356 - val_loss: 4.2617 - val_acc: 0.7350\n","Epoch 14/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2743 - acc: 0.7342 - val_loss: 4.0682 - val_acc: 0.7475\n","Epoch 15/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 4.2770 - acc: 0.7342 - val_loss: 4.7460 - val_acc: 0.7042\n","Epoch 16/20\n","4800/4800 [==============================] - 30s 6ms/step - loss: 4.3523 - acc: 0.7292 - val_loss: 4.0967 - val_acc: 0.7458\n","Epoch 17/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2221 - acc: 0.7377 - val_loss: 4.0984 - val_acc: 0.7450\n","Epoch 18/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1927 - acc: 0.7396 - val_loss: 4.1518 - val_acc: 0.7417\n","Epoch 19/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.2249 - acc: 0.7377 - val_loss: 4.2168 - val_acc: 0.7383\n","Epoch 20/20\n","4800/4800 [==============================] - 31s 6ms/step - loss: 4.1657 - acc: 0.7415 - val_loss: 4.1638 - val_acc: 0.7417\n","Training time: 611.545731306076\n","1200/1200 [==============================] - 8s 6ms/step\n","\n","layer_inx: -6\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 5,588,228\n","Non-trainable params: 12,354,880\n","_________________________________________________________________\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0203 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 2/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 3/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 4/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 5/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 6/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 7/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 8/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 9/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 10/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 11/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 12/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 13/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 14/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 15/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 16/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 17/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 18/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 19/20\n","4800/4800 [==============================] - 32s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Epoch 20/20\n","4800/4800 [==============================] - 31s 7ms/step - loss: 12.0617 - acc: 0.2517 - val_loss: 12.1960 - val_acc: 0.2433\n","Training time: 632.6831800937653\n","1200/1200 [==============================] - 8s 6ms/step\n","\n","layer_inx: -7\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_5 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 7,948,036\n","Non-trainable params: 9,995,072\n","_________________________________________________________________\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0462 - acc: 0.2504 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 2/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 3/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 4/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 5/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 6/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 7/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 8/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 9/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 10/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 11/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 12/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 13/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 14/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 15/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 16/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 17/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 18/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 19/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Epoch 20/20\n","4800/4800 [==============================] - 33s 7ms/step - loss: 12.1591 - acc: 0.2456 - val_loss: 11.8065 - val_acc: 0.2675\n","Training time: 658.4849185943604\n","1200/1200 [==============================] - 8s 6ms/step\n","\n","layer_inx: -8\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 10,307,844\n","Non-trainable params: 7,635,264\n","_________________________________________________________________\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 35s 7ms/step - loss: 12.0529 - acc: 0.2504 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 2/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 3/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 4/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 5/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 6/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 7/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 8/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 9/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 10/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 11/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 12/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 13/20\n","4800/4800 [==============================] - 35s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 14/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 15/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 16/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 17/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 18/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 19/20\n","4800/4800 [==============================] - 35s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 20/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Training time: 688.997477054596\n","1200/1200 [==============================] - 8s 6ms/step\n","\n","layer_inx: -9\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_7 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 10,307,844\n","Non-trainable params: 7,635,264\n","_________________________________________________________________\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 35s 7ms/step - loss: 12.0352 - acc: 0.2506 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 2/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 3/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 4/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 5/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 6/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 7/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 8/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 9/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 10/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 11/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 12/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 13/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 14/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 15/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 16/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 17/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 18/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 19/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 20/20\n","4800/4800 [==============================] - 34s 7ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Training time: 689.493414402008\n","1200/1200 [==============================] - 8s 6ms/step\n","\n","layer_inx: -10\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_8 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 128)               3211392   \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 128)               16512     \n","_________________________________________________________________\n","output (Dense)               (None, 4)                 516       \n","=================================================================\n","Total params: 17,943,108\n","Trainable params: 12,667,652\n","Non-trainable params: 5,275,456\n","_________________________________________________________________\n","Train on 4800 samples, validate on 1200 samples\n","Epoch 1/20\n","4800/4800 [==============================] - 38s 8ms/step - loss: 12.0331 - acc: 0.2512 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 2/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 3/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 4/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 5/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 6/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 7/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 8/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 9/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 10/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 11/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 12/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 13/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 14/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 15/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 16/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 17/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 18/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 19/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Epoch 20/20\n","4800/4800 [==============================] - 37s 8ms/step - loss: 12.0584 - acc: 0.2519 - val_loss: 12.2095 - val_acc: 0.2425\n","Training time: 735.4988322257996\n","1200/1200 [==============================] - 8s 7ms/step\n","\n"],"name":"stdout"}]},{"metadata":{"id":"S_2uet0_TrLi","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}